{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-04T15:08:48.480549Z",
     "iopub.status.busy": "2023-08-04T15:08:48.480177Z",
     "iopub.status.idle": "2023-08-04T15:08:48.489243Z",
     "shell.execute_reply": "2023-08-04T15:08:48.488315Z",
     "shell.execute_reply.started": "2023-08-04T15:08:48.480517Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras import backend, layers, metrics\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:18:53.415029Z",
     "iopub.status.busy": "2023-08-04T15:18:53.414665Z",
     "iopub.status.idle": "2023-08-04T15:18:53.422174Z",
     "shell.execute_reply": "2023-08-04T15:18:53.420975Z",
     "shell.execute_reply.started": "2023-08-04T15:18:53.415001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting seeds for reproducibility across runs\n",
    "random.seed(15)\n",
    "np.random.seed(15)\n",
    "tf.random.set_seed(15)\n",
    "\n",
    "# Setting the root path for the dataset\n",
    "ROOT = 'C:/Users/Mukul/Desktop/Mscdataset/Extracted_Faces/iris2'\n",
    "\n",
    "# Function to read and preprocess an image given its index\n",
    "def read_image(index):\n",
    "    path = os.path.join(ROOT, index[0], index[1])\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (128,128))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:18:54.296919Z",
     "iopub.status.busy": "2023-08-04T15:18:54.295641Z",
     "iopub.status.idle": "2023-08-04T15:18:54.463525Z",
     "shell.execute_reply": "2023-08-04T15:18:54.462496Z",
     "shell.execute_reply.started": "2023-08-04T15:18:54.296873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to split the dataset into training and testing sets\n",
    "def split_dataset(directory, split=0.9):\n",
    "    folders = [folder for folder in os.listdir(directory) if folder not in ['45', '46']]\n",
    "    num_train = int(len(folders)*split)\n",
    "    \n",
    "    random.shuffle(folders)\n",
    "    \n",
    "    train_list, test_list = {}, {}\n",
    "    \n",
    "    # Creating training list\n",
    "    for folder in folders[:num_train]:\n",
    "        num_files = len(os.listdir(os.path.join(directory, folder)))\n",
    "        train_list[folder] = num_files\n",
    "    \n",
    "    # Creating testing list\n",
    "    for folder in folders[num_train:]:\n",
    "        num_files = len(os.listdir(os.path.join(directory, folder)))\n",
    "        test_list[folder] = num_files  \n",
    "    \n",
    "    return train_list, test_list\n",
    "\n",
    "# Splitting the dataset into training and testing and printing their lengths\n",
    "train_list, test_list = split_dataset(ROOT, split=0.9)\n",
    "print(\"Length of training list:\", len(train_list))\n",
    "print(\"Length of testing list :\", len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:18:55.268436Z",
     "iopub.status.busy": "2023-08-04T15:18:55.267740Z",
     "iopub.status.idle": "2023-08-04T15:18:55.276523Z",
     "shell.execute_reply": "2023-08-04T15:18:55.275541Z",
     "shell.execute_reply.started": "2023-08-04T15:18:55.268401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create triplets for siamese network training\n",
    "def create_triplets(directory, folder_list, max_files=10):\n",
    "    triplets = []\n",
    "    folders = list(folder_list.keys())\n",
    "    \n",
    "    for folder in folders:\n",
    "        path = os.path.join(directory, folder)\n",
    "        files = [f for f in os.listdir(path) if f.endswith('.jpg')][:max_files]\n",
    "        num_files = len(files)\n",
    "        \n",
    "        for i in range(num_files-1):\n",
    "            for j in range(i+1, num_files):\n",
    "                anchor = (folder, files[i])\n",
    "                positive = (folder, files[j])\n",
    "\n",
    "                neg_folder = folder\n",
    "                while neg_folder == folder:\n",
    "                    neg_folder = random.choice(folders)\n",
    "                neg_path = os.path.join(directory, neg_folder)\n",
    "                neg_files = [f for f in os.listdir(neg_path) if f.endswith('.jpg')]\n",
    "                neg_file = random.choice(neg_files)\n",
    "                negative = (neg_folder, neg_file)\n",
    "\n",
    "                triplets.append((anchor, positive, negative))\n",
    "            \n",
    "    random.shuffle(triplets)\n",
    "    return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:18:56.098184Z",
     "iopub.status.busy": "2023-08-04T15:18:56.097126Z",
     "iopub.status.idle": "2023-08-04T15:18:56.239275Z",
     "shell.execute_reply": "2023-08-04T15:18:56.238291Z",
     "shell.execute_reply.started": "2023-08-04T15:18:56.098150Z"
    }
   },
   "outputs": [],
   "source": [
    "train_triplet = create_triplets(ROOT, train_list)\n",
    "test_triplet  = create_triplets(ROOT, test_list)\n",
    "\n",
    "print(\"Number of training triplets:\", len(train_triplet))\n",
    "print(\"Number of testing triplets :\", len(test_triplet))\n",
    "\n",
    "print(\"\\nExamples of triplets:\")\n",
    "for i in range(5):\n",
    "    print(train_triplet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:18:56.990416Z",
     "iopub.status.busy": "2023-08-04T15:18:56.987807Z",
     "iopub.status.idle": "2023-08-04T15:18:57.008196Z",
     "shell.execute_reply": "2023-08-04T15:18:57.007282Z",
     "shell.execute_reply.started": "2023-08-04T15:18:56.990377Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(triplet_list, batch_size=256, preprocess=True):\n",
    "    batch_steps = len(triplet_list)//batch_size\n",
    "    \n",
    "    for i in range(batch_steps+1):\n",
    "        anchor   = []\n",
    "        positive = []\n",
    "        negative = []\n",
    "        \n",
    "        j = i*batch_size\n",
    "        while j<(i+1)*batch_size and j<len(triplet_list):\n",
    "            a, p, n = triplet_list[j]\n",
    "            anchor.append(read_image(a))\n",
    "            positive.append(read_image(p))\n",
    "            negative.append(read_image(n))\n",
    "            j+=1\n",
    "            \n",
    "        anchor = np.array(anchor)\n",
    "        positive = np.array(positive)\n",
    "        negative = np.array(negative)\n",
    "        \n",
    "        if preprocess:\n",
    "            anchor = preprocess_input(anchor)\n",
    "            positive = preprocess_input(positive)\n",
    "            negative = preprocess_input(negative)\n",
    "        \n",
    "        yield ([anchor, positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:18:57.773914Z",
     "iopub.status.busy": "2023-08-04T15:18:57.772599Z",
     "iopub.status.idle": "2023-08-04T15:19:01.296397Z",
     "shell.execute_reply": "2023-08-04T15:19:01.295522Z",
     "shell.execute_reply.started": "2023-08-04T15:18:57.773869Z"
    }
   },
   "outputs": [],
   "source": [
    "num_plots = 6\n",
    "\n",
    "f, axes = plt.subplots(num_plots, 3, figsize=(15, 20))\n",
    "\n",
    "for x in get_batch(train_triplet, batch_size=num_plots, preprocess=False):\n",
    "    a,p,n = x\n",
    "    for i in range(num_plots):\n",
    "        axes[i, 0].imshow(a[i])\n",
    "        axes[i, 1].imshow(p[i])\n",
    "        axes[i, 2].imshow(n[i])\n",
    "        i+=1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:19:11.840204Z",
     "iopub.status.busy": "2023-08-04T15:19:11.839608Z",
     "iopub.status.idle": "2023-08-04T15:19:11.847881Z",
     "shell.execute_reply": "2023-08-04T15:19:11.846866Z",
     "shell.execute_reply.started": "2023-08-04T15:19:11.840170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract the encoder from the trained siamese model\n",
    "def get_encoder(input_shape):\n",
    "    pretrained_model = Xception(\n",
    "        input_shape=input_shape,\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        pooling='avg',\n",
    "    )\n",
    "    \n",
    "    for i in range(len(pretrained_model.layers)-27):\n",
    "        pretrained_model.layers[i].trainable = False\n",
    "\n",
    "    encode_model = Sequential([\n",
    "        pretrained_model,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
    "    ], name=\"Encode_Model\")\n",
    "    return encode_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:19:13.813276Z",
     "iopub.status.busy": "2023-08-04T15:19:13.812346Z",
     "iopub.status.idle": "2023-08-04T15:19:18.763440Z",
     "shell.execute_reply": "2023-08-04T15:19:18.762415Z",
     "shell.execute_reply.started": "2023-08-04T15:19:13.813237Z"
    }
   },
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "    \n",
    "\n",
    "def get_siamese_network(input_shape = (128, 128, 3)):\n",
    "    encoder = get_encoder(input_shape)\n",
    "    \n",
    "    # Input Layers for the images\n",
    "    anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n",
    "    positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n",
    "    negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n",
    "    \n",
    "    ## Generate the encodings (feature vectors) for the images\n",
    "    encoded_a = encoder(anchor_input)\n",
    "    encoded_p = encoder(positive_input)\n",
    "    encoded_n = encoder(negative_input)\n",
    "    \n",
    "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
    "    distances = DistanceLayer()(\n",
    "        encoder(anchor_input),\n",
    "        encoder(positive_input),\n",
    "        encoder(negative_input)\n",
    "    )\n",
    "    \n",
    "    # Defining the siamese model architecture\n",
    "    siamese_network = Model(\n",
    "        inputs  = [anchor_input, positive_input, negative_input],\n",
    "        outputs = distances,\n",
    "        name = \"Siamese_Network\"\n",
    "    )\n",
    "    return siamese_network\n",
    "\n",
    "siamese_network = get_siamese_network()\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:19:18.766431Z",
     "iopub.status.busy": "2023-08-04T15:19:18.765718Z",
     "iopub.status.idle": "2023-08-04T15:19:18.844667Z",
     "shell.execute_reply": "2023-08-04T15:19:18.843671Z",
     "shell.execute_reply.started": "2023-08-04T15:19:18.766396Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(siamese_network, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:19:18.846734Z",
     "iopub.status.busy": "2023-08-04T15:19:18.846098Z",
     "iopub.status.idle": "2023-08-04T15:19:18.857145Z",
     "shell.execute_reply": "2023-08-04T15:19:18.856076Z",
     "shell.execute_reply.started": "2023-08-04T15:19:18.846697Z"
    }
   },
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    # Builds a Siamese model based on a base-model\n",
    "    def __init__(self, siamese_network, margin=1.0):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        \n",
    "        self.margin = margin\n",
    "        self.siamese_network = siamese_network\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape get the gradients when we compute loss, and uses them to update the weights\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "    def _compute_loss(self, data):\n",
    "        # Get the two distances from the network, then compute the triplet loss\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:19:18.860068Z",
     "iopub.status.busy": "2023-08-04T15:19:18.859414Z",
     "iopub.status.idle": "2023-08-04T15:19:18.885683Z",
     "shell.execute_reply": "2023-08-04T15:19:18.884773Z",
     "shell.execute_reply.started": "2023-08-04T15:19:18.860031Z"
    }
   },
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\n",
    "siamese_model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:19:21.465567Z",
     "iopub.status.busy": "2023-08-04T15:19:21.465207Z",
     "iopub.status.idle": "2023-08-04T15:19:21.472608Z",
     "shell.execute_reply": "2023-08-04T15:19:21.471690Z",
     "shell.execute_reply.started": "2023-08-04T15:19:21.465535Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_on_triplets(batch_size = 256):\n",
    "    pos_scores, neg_scores = [], []\n",
    "\n",
    "    for data in get_batch(test_triplet, batch_size=batch_size):\n",
    "        prediction = siamese_model.predict(data)\n",
    "        pos_scores += list(prediction[0])\n",
    "        neg_scores += list(prediction[1])\n",
    "    \n",
    "    accuracy = np.sum(np.array(pos_scores) < np.array(neg_scores)) / len(pos_scores)\n",
    "    ap_mean = np.mean(pos_scores)\n",
    "    an_mean = np.mean(neg_scores)\n",
    "    ap_stds = np.std(pos_scores)\n",
    "    an_stds = np.std(neg_scores)\n",
    "    \n",
    "    print(f\"Accuracy on test = {accuracy:.5f}\")\n",
    "    return (accuracy, ap_mean, an_mean, ap_stds, an_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:19:23.193126Z",
     "iopub.status.busy": "2023-08-04T15:19:23.192467Z",
     "iopub.status.idle": "2023-08-04T15:37:25.812323Z",
     "shell.execute_reply": "2023-08-04T15:37:25.811347Z",
     "shell.execute_reply.started": "2023-08-04T15:19:23.193091Z"
    }
   },
   "outputs": [],
   "source": [
    "save_all = False\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "max_acc = 0\n",
    "train_loss = []\n",
    "test_metrics = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    t = time.time()\n",
    "    \n",
    "    # Training the model on train data\n",
    "    epoch_loss = []\n",
    "    for data in get_batch(train_triplet, batch_size=batch_size):\n",
    "        loss = siamese_model.train_on_batch(data)\n",
    "        epoch_loss.append(loss)\n",
    "    epoch_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    train_loss.append(epoch_loss)\n",
    "\n",
    "    print(f\"\\nEPOCH: {epoch} \\t (Epoch done in {int(time.time()-t)} sec)\")\n",
    "    print(f\"Loss on train    = {epoch_loss:.5f}\")\n",
    "    \n",
    "    # Testing the model on test data\n",
    "    metric = test_on_triplets(batch_size=batch_size)\n",
    "    test_metrics.append(metric)\n",
    "    accuracy = metric[0]\n",
    "    \n",
    "    # Saving the model weights\n",
    "    if save_all or accuracy>=max_acc:\n",
    "        siamese_model.save_weights(\"siamese_model\")\n",
    "        max_acc = accuracy\n",
    "        \n",
    "# Saving the model after all epochs run\n",
    "siamese_model.save(\"iris_siamese_model-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:41:19.904494Z",
     "iopub.status.busy": "2023-08-04T15:41:19.904109Z",
     "iopub.status.idle": "2023-08-04T15:41:21.102596Z",
     "shell.execute_reply": "2023-08-04T15:41:21.101651Z",
     "shell.execute_reply.started": "2023-08-04T15:41:19.904463Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(loss, metrics):\n",
    "    # Extracting individual metrics from metrics\n",
    "    accuracy = metrics[:, 0]\n",
    "    ap_mean  = metrics[:, 1]\n",
    "    an_mean  = metrics[:, 2]\n",
    "    ap_stds  = metrics[:, 3]\n",
    "    an_stds  = metrics[:, 4]\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    # Plotting the loss over epochs\n",
    "    plt.subplot(121)\n",
    "    plt.plot(loss, 'b', label='Loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the accuracy over epochs\n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuracy, 'r', label='Accuracy')\n",
    "    plt.title('Testing Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    # Comparing the Means over epochs\n",
    "    plt.subplot(121)\n",
    "    plt.plot(ap_mean, 'b', label='AP Mean')\n",
    "    plt.plot(an_mean, 'g', label='AN Mean')\n",
    "    plt.title('Means Comparision')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the accuracy\n",
    "    ap_75quartile = (ap_mean+ap_stds)\n",
    "    an_75quartile = (an_mean-an_stds)\n",
    "    plt.subplot(122)\n",
    "    plt.plot(ap_75quartile, 'b', label='AP (Mean+SD)')\n",
    "    plt.plot(an_75quartile, 'g', label='AN (Mean-SD)')\n",
    "    plt.title('75th Quartile Comparision')\n",
    "    plt.legend()\n",
    "\n",
    "test_metrics = np.array(test_metrics)\n",
    "plot_metrics(train_loss, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:41:28.202238Z",
     "iopub.status.busy": "2023-08-04T15:41:28.201869Z",
     "iopub.status.idle": "2023-08-04T15:41:30.842996Z",
     "shell.execute_reply": "2023-08-04T15:41:30.841998Z",
     "shell.execute_reply.started": "2023-08-04T15:41:28.202206Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_encoder(model):\n",
    "    encoder = get_encoder((128, 128, 3))\n",
    "    i=0\n",
    "    for e_layer in model.layers[0].layers[3].layers:\n",
    "        layer_weight = e_layer.get_weights()\n",
    "        encoder.layers[i].set_weights(layer_weight)\n",
    "        i+=1\n",
    "    return encoder\n",
    "# Extracting the encoder and saving its weights\n",
    "encoder = extract_encoder(siamese_model)\n",
    "encoder.save_weights(\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:41:33.411319Z",
     "iopub.status.busy": "2023-08-04T15:41:33.410594Z",
     "iopub.status.idle": "2023-08-04T15:41:33.417856Z",
     "shell.execute_reply": "2023-08-04T15:41:33.415934Z",
     "shell.execute_reply.started": "2023-08-04T15:41:33.411282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to classify image pairs as similar or different\n",
    "def classify_images(iris_list1, iris_list2, threshold=1.3):\n",
    "    # Getting the encodings for the passed irises\n",
    "    tensor1 = encoder.predict(iris_list1)\n",
    "    tensor2 = encoder.predict(iris_list2)\n",
    "    \n",
    "    distance = np.sum(np.square(tensor1-tensor2), axis=-1)\n",
    "    prediction = np.where(distance<=threshold, 0, 1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T15:41:36.363224Z",
     "iopub.status.busy": "2023-08-04T15:41:36.362457Z",
     "iopub.status.idle": "2023-08-04T15:41:41.418610Z",
     "shell.execute_reply": "2023-08-04T15:41:41.417627Z",
     "shell.execute_reply.started": "2023-08-04T15:41:36.363181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to compute and display model metrics\n",
    "def ModelMetrics(pos_list, neg_list):\n",
    "    true = np.array([0]*len(pos_list)+[1]*len(neg_list))\n",
    "    pred = np.append(pos_list, neg_list)\n",
    "    \n",
    "    # Compute and print the accuracy\n",
    "    print(f\"\\nAccuracy of model: {accuracy_score(true, pred)}\\n\")\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(true, pred)\n",
    "\n",
    "    categories  = ['Similar','Different']\n",
    "    names = ['True Similar','False Similar', 'False Different','True Different']\n",
    "    percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
    "\n",
    "# Applying the model to a batch of test triplets to get predictions\n",
    "pos_list = np.array([])\n",
    "neg_list = np.array([])\n",
    "for data in get_batch(test_triplet, batch_size=256):\n",
    "    a, p, n = data\n",
    "    pos_list = np.append(pos_list, classify_images(a, p))\n",
    "    neg_list = np.append(neg_list, classify_images(a, n))\n",
    "    break\n",
    "    \n",
    "# Evaluating the model's predictions\n",
    "ModelMetrics(pos_list, neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
